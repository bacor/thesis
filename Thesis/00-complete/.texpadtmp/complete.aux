\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@sortscheme{nyt}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\@ifundefined {etoctocstyle}{\let \etoc@startlocaltoc \@gobble \let \etoc@settocdepth \@gobble \let \etoc@depthtag \@gobble }{}}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\select@language{english}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}The cultural origins of language}{7}{chapter.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:The cultural origins of language}{{1}{7}{The cultural origins of language}{chapter.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\etoc@startlocaltoc{2}}
\abx@aux@cite{Zuidema2013}
\abx@aux@cite{Hockett1960}
\abx@aux@cite{Kirby2017}
\abx@aux@cite{Berwick2017}
\abx@aux@cite{Christiansen2016a}
\abx@aux@page{1}{8}
\abx@aux@page{2}{8}
\abx@aux@page{3}{8}
\abx@aux@page{4}{8}
\abx@aux@cite{Kirby2002}
\abx@aux@cite{Smith2014}
\abx@aux@cite{Steels2016}
\abx@aux@cite{Christiansen2016}
\abx@aux@cite{Tomasello1999}
\abx@aux@cite{Jaeger2009}
\abx@aux@page{5}{9}
\abx@aux@page{6}{9}
\abx@aux@page{7}{9}
\abx@aux@page{8}{9}
\abx@aux@page{9}{9}
\abx@aux@page{10}{9}
\abx@aux@page{11}{9}
\abx@aux@page{12}{9}
\abx@aux@page{13}{9}
\abx@aux@page{14}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Source code and data}{10}{section*.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline notation}{11}{section*.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Iterated Learning}{13}{chapter.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:iterated-learning}{{2}{13}{Iterated Learning}{chapter.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\etoc@startlocaltoc{3}}
\abx@aux@cite{Brighton2002}
\abx@aux@cite{Chomsky1986}
\abx@aux@cite{Kirby2001}
\abx@aux@cite{Hurford2000}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 59}}{14}{Early iterated learning models}{figure.caption.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces In the iterated learning model, the language produced by the previous generation serves as the primary linguistic data for the next. \par \vspace  {.5em}{\color  {lorange}Adapted from \textcite {Kirby2001}.} \relax }}{14}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:iterated-learning-illustration}{{\relax 2.1}{14}{In the iterated learning model, the language produced by the previous generation serves as the primary linguistic data for the next. \figdetails {Adapted from \textcite {Kirby2001}.} \relax }{figure.caption.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Early iterated learning models}{14}{section.2.1}}
\abx@aux@page{15}{14}
\abx@aux@page{16}{14}
\oddpage@label{1}{14}
\newlabel{fn:mind-read}{{1}{14}{Early iterated learning models}{mfootnote.1}{}}
\pgfsyspdfmark {pgfid1}{33337098}{29436305}
\abx@aux@page{19}{14}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline the emergence of compositionality, i}{14}{section*.4}}
\abx@aux@page{20}{14}
\abx@aux@cite{Cornish2011}
\abx@aux@cite{Zuidema2003}
\abx@aux@cite{Smith2002a}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Transmission bottlenecks and generalisation}{15}{section*.5}}
\oddpage@label{2}{15}
\abx@aux@page{21}{15}
\pgfsyspdfmark {pgfid3}{10886142}{28521400}
\abx@aux@page{22}{15}
\abx@aux@page{23}{15}
\abx@aux@page{24}{15}
\abx@aux@page{25}{15}
\abx@aux@page{26}{15}
\abx@aux@page{27}{15}
\abx@aux@page{28}{15}
\abx@aux@page{29}{15}
\abx@aux@cite{Culbertson2016}
\abx@aux@cite{Griffiths2005}
\abx@aux@cite{Griffiths2007}
\abx@aux@cite{Perfors2011}
\abx@aux@cite{Goodman2016}
\abx@aux@cite{Griffiths2008}
\abx@aux@cite{Kirby2014}
\abx@aux@cite{Griffiths2007a}
\abx@aux@page{30}{16}
\abx@aux@page{31}{16}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Iterated learning with Bayesian agents}{16}{section.2.2}}
\abx@aux@page{32}{16}
\abx@aux@page{33}{16}
\abx@aux@page{34}{16}
\abx@aux@page{35}{16}
\abx@aux@page{36}{16}
\oddpage@label{3}{16}
\pgfsyspdfmark {pgfid4}{13824485}{18383561}
\abx@aux@page{37}{16}
\abx@aux@cite{Kirby2004}
\abx@aux@cite{Kirby2007}
\abx@aux@cite{Burkett2010}
\abx@aux@cite{Kirby2015}
\newlabel{SC@2}{{\caption@xref {??}{ on input line 266}}{17}{Iterated learning with Bayesian agents}{figure.caption.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.2}{\ignorespaces Exponentiating a distribution moves the probability mass towards the mode. Illustrated for three different distributions. \par \vspace  {.5em}{\color  {lorange}\textsc  {\textbf  {\lowercase {fig03}}}} \relax }}{17}{figure.caption.6}}
\newlabel{fig:FIG03}{{\relax 2.2}{17}{Exponentiating a distribution moves the probability mass towards the mode. Illustrated for three different distributions. \figdetails {\figid {fig03}} \relax }{figure.caption.6}{}}
\abx@aux@page{38}{17}
\abx@aux@page{39}{17}
\abx@aux@page{40}{17}
\abx@aux@page{41}{17}
\abx@aux@page{42}{17}
\abx@aux@page{43}{17}
\abx@aux@page{44}{17}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline The emergence of compositionality, ii}{18}{section*.7}}
\abx@aux@page{45}{18}
\abx@aux@page{46}{18}
\abx@aux@page{47}{18}
\abx@aux@page{48}{18}
\newlabel{SC@3}{{\caption@xref {??}{ on input line 354}}{19}{The emergence of compositionality, ii}{figure.caption.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.3}{\ignorespaces  Emergence of compositionality in the Bayesian iterated learning model of \textcite {Griffiths2007a}. On the left, the language used in every generation with H one of 252 holistic languages and C1--4 the compositional languages. On the right the relative frequency of every language up to a certain time $t$. These relative frequencies converge to the prior (orange). Larger bottlenecks (subfigures A--C) slow down convergence. \par \vspace  {.5em}{\color  {lorange}\textsc  {\textbf  {\lowercase {gk01}}} WebPPL simulation with ${"α}=0.5$, ${"ε}=0.001$ and samplers (${"η}=1$).} \relax }}{19}{figure.caption.8}}
\newlabel{fig:GK01-bottlenecks}{{\relax 2.3}{19}{Emergence of compositionality in the Bayesian iterated learning model of \textcite {Griffiths2007a}. On the left, the language used in every generation with H one of 252 holistic languages and C1--4 the compositional languages. On the right the relative frequency of every language up to a certain time $t$. These relative frequencies converge to the prior (orange). Larger bottlenecks (subfigures A--C) slow down convergence. \figdetails {\figid {gk01} WebPPL simulation with $\alpha =0.5$, $\epsilon =0.001$ and samplers ($\eta =1$).} \relax }{figure.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convergence to the prior}{19}{section.2.3}}
\newlabel{eq:iterated-learning-chain}{{\relax 2.7}{19}{Convergence to the prior}{equation.2.3.7}{}}
\abx@aux@page{51}{19}
\newlabel{SC@4}{{\caption@xref {??}{ on input line 428}}{20}{Convergence to the prior}{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.4}{\ignorespaces Different Markov chains hidden in the Bayesian iterated learning model, and to which stationary distribution they converge (right). \par \vspace  {.5em}{\color  {lorange}Figure adapted from \textcite {Griffiths2007a}.} \relax }}{20}{figure.caption.9}}
\newlabel{fig:gk-markov-chains}{{\relax 2.4}{20}{Different Markov chains hidden in the Bayesian iterated learning model, and to which stationary distribution they converge (right). \figdetails {Figure adapted from \textcite {Griffiths2007a}.} \relax }{figure.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Proof of the convergence to the prior}{20}{section*.10}}
\abx@aux@page{54}{20}
\newlabel{eq:mc-interpretations}{{\relax 2.8}{20}{Proof of the convergence to the prior}{equation.2.3.8}{}}
\newlabel{eq:to-prove-convergence}{{\relax 2.9}{20}{Proof of the convergence to the prior}{equation.2.3.9}{}}
\newlabel{eq:proof-conv-to-the-prior}{{\relax 2.10}{21}{Proof of the convergence to the prior}{equation.2.3.10}{}}
\newlabel{eq:chain-joint}{{\relax 2.13}{21}{Proof of the convergence to the prior}{equation.2.3.13}{}}
\abx@aux@page{55}{21}
\abx@aux@cite{Smith2009}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convergence to the maximum of the prior?}{22}{section*.11}}
\abx@aux@page{56}{22}
\oddpage@label{4}{22}
\pgfsyspdfmark {pgfid6}{21950775}{42189428}
\abx@aux@page{57}{22}
\abx@aux@page{58}{22}
\abx@aux@page{59}{22}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Convergent controversy}{22}{section.2.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Bottlenecks and weak biases}{22}{section*.12}}
\abx@aux@page{60}{22}
\abx@aux@cite{Nowak2001}
\abx@aux@cite{Niyogi2009}
\abx@aux@page{61}{23}
\abx@aux@page{62}{23}
\abx@aux@page{63}{23}
\abx@aux@page{64}{23}
\oddpage@label{5}{23}
\abx@aux@page{65}{23}
\abx@aux@page{66}{23}
\pgfsyspdfmark {pgfid7}{9689868}{30360045}
\abx@aux@page{67}{23}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline population structure and heterogenous populations}{23}{section*.13}}
\abx@aux@page{68}{23}
\abx@aux@page{69}{23}
\abx@aux@page{70}{23}
\abx@aux@page{71}{23}
\abx@aux@cite{Ferdinand2009}
\abx@aux@cite{Dediu2009}
\abx@aux@cite{Whalen2017}
\abx@aux@page{72}{24}
\abx@aux@page{73}{24}
\abx@aux@page{74}{24}
\abx@aux@page{75}{24}
\abx@aux@page{76}{24}
\abx@aux@page{77}{24}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Lineages and cumulative cultural evolution}{24}{section*.14}}
\abx@aux@page{78}{24}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Conclusions}{25}{section.2.5}}
\newlabel{desideratum:biases}{{{{(\textsc  {d}1)}}}{25}{Conclusions}{Item.3}{}}
\newlabel{desideratum:strategies}{{{{(\textsc  {d}2)}}}{25}{Conclusions}{Item.4}{}}
\newlabel{desideratum:analysable}{{{{(\textsc  {d}3)}}}{25}{Conclusions}{Item.5}{}}
\newlabel{desideratum:cultural-effects}{{{{(\textsc  {d}4)}}}{25}{Conclusions}{Item.6}{}}
\newlabel{desideratum:robust}{{{{(\textsc  {d}5)}}}{25}{Conclusions}{Item.7}{}}
\newlabel{desideratum:stable}{{{{(\textsc  {d}6)}}}{26}{Conclusions}{Item.8}{}}
\abx@aux@page{79}{26}
\newlabel{desideratum:empirical}{{{{(\textsc  {d}7)}}}{26}{Conclusions}{Item.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Naming Games}{27}{chapter.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:naming-games}{{3}{27}{Naming Games}{chapter.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\etoc@startlocaltoc{4}}
\abx@aux@cite{Steels1995}
\abx@aux@cite{Steels2011}
\abx@aux@cite{Steels2012}
\abx@aux@cite{Steels2015}
\abx@aux@cite{DeVylder2006}
\abx@aux@cite{Wellens2012}
\abx@aux@page{80}{28}
\abx@aux@page{81}{28}
\abx@aux@page{82}{28}
\abx@aux@page{83}{28}
\abx@aux@page{84}{28}
\abx@aux@page{85}{28}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}The basic naming game}{28}{section.3.1}}
\abx@aux@page{86}{28}
\oddpage@label{6}{28}
\pgfsyspdfmark {pgfid8}{16387316}{10810169}
\abx@aux@cite{Oliphant1996}
\abx@aux@cite{Baronchelli2006}
\abx@aux@cite{Baronchelli2006a}
\newlabel{SC@5}{{\caption@xref {??}{ on input line 165}}{29}{The minimal strategy}{figure.caption.15}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces The updates of the minimal naming game illustrated. If communication fails, the hearer adds the word uttered by the speaker (bold) to its vocabulary. After a success, both empty their vocabularies and keep only the communicated word. \par \vspace  {.5em}{\color  {lorange}Figure inspired by \textcite {Wellens2012}.} \relax }}{29}{figure.caption.15}}
\newlabel{fig:ch3:minimal-naming-game-updates}{{\relax 3.1}{29}{The updates of the minimal naming game illustrated. If communication fails, the hearer adds the word uttered by the speaker (bold) to its vocabulary. After a success, both empty their vocabularies and keep only the communicated word. \figdetails {Figure inspired by \textcite {Wellens2012}.} \relax }{figure.caption.15}{}}
\abx@aux@page{87}{29}
\abx@aux@page{88}{29}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}The minimal strategy}{29}{section.3.2}}
\abx@aux@page{89}{29}
\abx@aux@cite{Baronchelli2017}
\abx@aux@page{92}{30}
\abx@aux@page{93}{30}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Phenomenology}{30}{section*.16}}
\abx@aux@page{94}{30}
\abx@aux@cite{Loreto2011}
\abx@aux@cite{DallAsta2006}
\newlabel{SC@6}{{\caption@xref {??}{ on input line 234}}{31}{Phenomenology}{figure.caption.17}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces  The dynamics of the minimal naming game. An sharp transition leads to convergence and the emergence of consensus. \par \vspace  {.5em}{\color  {lorange}\textsc  {\textbf  {\lowercase {MNG01}}} Results shown for $N=200$; avg.\ of 300 runs, 1 std.\ shaded.} \relax }}{31}{figure.caption.17}}
\newlabel{fig:MNG01-results}{{\relax 3.2}{31}{The dynamics of the minimal naming game. An sharp transition leads to convergence and the emergence of consensus. \figdetails {\figid {MNG01} Results shown for $N=200$; avg.\ of 300 runs, 1 std.\ shaded.} \relax }{figure.caption.17}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Scaling relations and network structure}{31}{section*.18}}
\abx@aux@page{95}{31}
\abx@aux@page{96}{31}
\newlabel{eq:ch3:scaling-naming-game}{{\relax 3.1}{31}{Scaling relations and network structure}{equation.3.2.1}{}}
\abx@aux@page{97}{31}
\abx@aux@page{98}{31}
\abx@aux@page{99}{31}
\abx@aux@page{100}{31}
\abx@aux@cite{Steels2005}
\newlabel{SC@7}{{\caption@xref {??}{ on input line 347}}{32}{Lateral inhibition strategies}{table.caption.19}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {\relax 3.1}{\ignorespaces Parameter settings for four different strategies, whose behaviour is shown in figure \ref  {fig:LING01-strategies}. Note that equivalent parametrisations also exist; see main text for details. \relax }}{32}{table.caption.19}}
\newlabel{table:li-strategies}{{\relax 3.1}{32}{Parameter settings for four different strategies, whose behaviour is shown in figure \ref {fig:LING01-strategies}. Note that equivalent parametrisations also exist; see main text for details. \relax }{table.caption.19}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Lateral inhibition strategies}{32}{section.3.3}}
\abx@aux@page{101}{32}
\abx@aux@page{102}{32}
\abx@aux@page{103}{32}
\oddpage@label{7}{32}
\abx@aux@page{104}{32}
\pgfsyspdfmark {pgfid9}{30832391}{30809053}
\abx@aux@page{105}{32}
\newlabel{SC@8}{{\caption@xref {??}{ on input line 363}}{33}{Lateral inhibition strategies}{figure.caption.20}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces  Comparison of the four naming game strategies in table \ref  {table:li-strategies}. The the unique word count and communicative success show that all strategies reach communicative success. The stable language for the frequency strategy is not efficient. \par \vspace  {.5em}{\color  {lorange}\textsc  {\textbf  {\lowercase {LING01}}} Results shown for $N=200$; avg.\ of 300 runs. $p_{\text  {success}}$ is a rolling average over a centered window of 1000 iterations.} \relax }}{33}{figure.caption.20}}
\newlabel{fig:LING01-strategies}{{\relax 3.3}{33}{Comparison of the four naming game strategies in table \ref {table:li-strategies}. The the unique word count and communicative success show that all strategies reach communicative success. The stable language for the frequency strategy is not efficient. \figdetails {\figid {LING01} Results shown for $N=200$; avg.\ of 300 runs. $p_{\text {success}}$ is a rolling average over a centered window of 1000 iterations.} \relax }{figure.caption.20}{}}
\abx@aux@page{106}{33}
\abx@aux@page{107}{33}
\oddpage@label{8}{33}
\abx@aux@page{108}{33}
\pgfsyspdfmark {pgfid10}{21776143}{23109119}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}Proof of convergence}{33}{section.3.4}}
\abx@aux@page{109}{33}
\newlabel{SC@9}{{\caption@xref {??}{ on input line 438}}{34}{Proof of convergence}{figure.caption.21}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.4}{\ignorespaces A discrete distribution $\boldsymbol  {\mathbf  {{"θ}}}$ over three values corresponds to a point in the $2$-simplex, a triangular slice of $\mathbb  {R}^3$ (left). The simplex can be embedded in the plane (middle), so that every point in the triangle determines a distribution (right).  \relax }}{34}{figure.caption.21}}
\newlabel{fig:ch3:simplex}{{\relax 3.4}{34}{A discrete distribution $\vlang $ over three values corresponds to a point in the $2$-simplex, a triangular slice of $\mathbb {R}^3$ (left). The simplex can be embedded in the plane (middle), so that every point in the triangle determines a distribution (right).  \relax }{figure.caption.21}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\nonumberline Preliminaries}{34}{section*.22}}
\abx@aux@page{110}{34}
\oddpage@label{9}{34}
\abx@aux@page{111}{34}
\pgfsyspdfmark {pgfid13}{29222059}{24010458}
\newlabel{SC@10}{{\caption@xref {??}{ on input line 491}}{35}{Preliminari